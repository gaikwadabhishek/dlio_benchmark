model: 
  name: aistore_1gb
  type: cnn

framework: pytorch

workflow:
  generate_data: True
  train: True
  checkpoint: False
  evaluation: True

dataset: 
  data_folder: ais://dlio-benchmark-1gb
  format: npy
  num_files_train: 256        # 256 files for training
  num_files_eval: 64          # 64 files for evaluation
  num_samples_per_file: 1     # 1 sample per file
  record_length_bytes: 4194304  # 4MB per sample (256 * 4MB = 1GB train, 64 * 4MB = 256MB eval)
  record_length_bytes_stdev: 0
  record_length_bytes_resize: 8388608  # 8MB resize buffer
  num_subfolders_train: 0
  num_subfolders_eval: 0

storage:
  storage_type: aistore
  storage_root: dlio-benchmark-1gb
  storage_options:
    endpoint_url: http://aistore-proxy:8080

reader: 
  data_loader: pytorch
  batch_size: 8              # Larger batch for bigger dataset
  batch_size_eval: 4
  read_threads: 8            # More threads for parallel reading
  file_shuffle: seed
  sample_shuffle: seed
  prefetch_size: 4           # Larger prefetch buffer

train:
  epochs: 5                  # More epochs for realistic benchmark
  computation_time: 0.5

evaluation: 
  eval_time: 0.25
  epochs_between_evals: 1

checkpoint:
  checkpoint_folder: checkpoints/aistore_1gb
  checkpoint_after_epoch: 5
  epochs_between_checkpoints: 2



